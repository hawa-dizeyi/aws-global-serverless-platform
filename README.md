# ğŸŒ Project 02 â€” Global Serverless Platform (Multi-Region, Active-Active)

This project demonstrates the design and implementation of a **globally distributed, serverless backend on AWS**, built with **Terraform** and following **production-grade cloud engineering practices**.

The platform is **multi-region (active-active)**, highly available, cost-aware, and designed for **zero-downtime regional resilience** using DNS-based traffic steering and health-checkâ€“driven failover.

---

## ğŸ¯ Project Goals

- Build a **multi-region serverless backend** using AWS native services
- Demonstrate **true active-active architecture** across regions
- Implement infrastructure as code with **Terraform**
- Apply **cost controls and safe defaults**
- Produce **verifiable deployment proof** suitable for a senior-level portfolio

---

## ğŸŒ Region Strategy

| Role       | AWS Region |
|-----------|------------|
| Primary   | eu-west-1 (Europe â€“ Ireland) |
| Secondary | eu-central-1 (Europe â€“ Frankfurt) |

**Why this pairing:**
- Low latency for EU users
- Strong regional separation
- Common real-world production pairing
- Full service parity for serverless workloads

---

## ğŸ—ï¸ Architecture (Current State)

### 1ï¸âƒ£ Infrastructure Foundation

- Terraform multi-provider setup
- Explicit primary / secondary provider aliasing
- Environment-scoped configuration (`environments/dev`)
- Centralized naming and tagging strategy
- Feature flags for optional services (CloudFront, WAF, Route 53 health checks)
- Safe defaults to reduce accidental cost or blast radius

---

### 2ï¸âƒ£ Global Data Layer â€” DynamoDB Global Tables

- DynamoDB Global Table spanning:
  - eu-west-1
  - eu-central-1
- `PAY_PER_REQUEST` billing (no capacity planning)
- TTL enabled for automatic cleanup of test data
- DynamoDB Streams enabled for global replication

**Result:**  
Writes in one region automatically replicate to the other without custom code.

ğŸ“¸ Screenshots: `screenshots/dynamodb/`

---

### 3ï¸âƒ£ Compute Layer â€” AWS Lambda (Multi-Region)

- Identical Lambda functions deployed independently in both regions
- Runtime: **Python 3.12**
- ZIP-based deployment using Terraform (`archive_file`)
- Shared execution role with **least-privilege** DynamoDB access
- Short CloudWatch log retention (cost-controlled)
- No reserved concurrency (avoids account-level constraints)

**Endpoints:**
- `GET /health` â€” regional health response
- `POST /write` â€” writes to DynamoDB (replicated globally)

ğŸ“¸ Screenshots: `screenshots/lambda/`

---

### 4ï¸âƒ£ API Layer â€” API Gateway (Active-Active)

- HTTP API Gateway deployed independently per region
- Regional APIs integrated with regional Lambdas
- Identical routes in each region
- Throttling and safe defaults enabled
- No cross-region dependencies

ğŸ“¸ Screenshots: `screenshots/api-gateway/`

---

### 5ï¸âƒ£ Global Traffic Management â€” Route 53 (Failover + Health Checks)

- Public hosted zone: **hawser-labs.online**
- API subdomain: **api.hawser-labs.online**
- **Failover routing policy** (PRIMARY / SECONDARY)
- HTTPS health checks against `/health` per region
- Automatic removal of unhealthy region from DNS
- IPv4 (`A`) and IPv6 (`AAAA`) records

ğŸ“¸ Screenshots: `screenshots/route53/`

---

## ğŸ” Regional Isolation & Active-Active Replication (Proof)

This section provides **explicit, reproducible proof** of active-active behavior and regional isolation.

### âœ… Independent Regional Health

Each region responds independently:

- eu-west-1 reports `region: eu-west-1`
- eu-central-1 reports `region: eu-central-1`

ğŸ“¸ Evidence:
- `screenshots/dynamodb/regional-health-eu.png`
- `screenshots/dynamodb/regional-health-de.png`

---

### âœ… Cross-Region Data Replication

1. Write executed in **Ireland**
2. Item read from **Frankfurt**
3. Write executed in **Frankfurt**
4. Item read from **Ireland**

Replication is handled entirely by DynamoDB Global Tables.

ğŸ“¸ Evidence:
- `screenshots/dynamodb/write-eu-output.png`
- `screenshots/dynamodb/get-from-de-eu-item.png`
- `screenshots/dynamodb/write-de-output.png`
- `screenshots/dynamodb/get-from-eu-de-item.png`

---

### âœ… Automated Failover (Controlled Failure)

- Health check intentionally failed for PRIMARY (eu-west-1)
- Route 53 marked PRIMARY as unhealthy
- DNS automatically routed traffic to SECONDARY (eu-central-1)
- No client-side changes required

ğŸ“¸ Evidence:
- `screenshots/route53/failover-routing.png`
- `screenshots/route53/primary-unhealthy.png`

---

### âœ… Frontend Validation (CORS + Visibility)

A minimal public UI is hosted on **Vercel** (`ui.hawser-labs.online`) to:

- Justify **strict CORS** configuration
- Visualize routing decisions
- Display:
  - Region badge
  - Request latency
  - Failover detection banner
  - Animated response cards

ğŸ“¸ Evidence:
- `screenshots/frontend/frontend-global-health.png`
- `screenshots/frontend/frontend-latency.png`
- `screenshots/frontend/frontend-failover-banner.png`

---

## ğŸ” Security & Cost Controls

- No static credentials committed
- IAM least-privilege policies
- No EC2, NAT Gateways, or always-on infrastructure
- Short log retention across services
- Strict CORS (no wildcard origins)
- Feature-flagged edge services (CloudFront, WAF)
- Clean, reproducible Terraform state

---

## ğŸ“ Repository Structure

~~~text
.
â”œâ”€â”€ environments/
â”‚   â””â”€â”€ dev/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ providers.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â””â”€â”€ terraform.tfvars.example
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ dynamodb/
â”‚   â”œâ”€â”€ lambda/
â”‚   â””â”€â”€ api/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ lambda/
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ dynamodb/
â”‚   â”œâ”€â”€ lambda/
â”‚   â”œâ”€â”€ api-gateway/
â”‚   â”œâ”€â”€ route53/
â”‚   â””â”€â”€ frontend/
â””â”€â”€ README.md
~~~

---

## ğŸ“Œ Key Engineering Takeaways

- Designed for failure by default (no single-region dependency)
- True active-active architecture with independent regional stacks
- Health-check driven DNS failover using Route 53
- Global replication handled by DynamoDB Global Tables (not custom code)
- Cost-aware defaults: no NAT/EC2, short log retention, feature flags
- Incremental rollout with low blast radius (enable features in phases)
- Terraform configuration structured for maintainability and reuse (modules + envs)

---

## ğŸ§¹ Cleanup

All infrastructure can be removed safely with:

~~~text
terraform destroy
~~~

This project avoids hidden dependencies and is designed to teardown cleanly.

---

## ğŸ“ Notes

This repository is part of a cloud engineering portfolio and intentionally prioritizes:
- clarity over complexity
- correctness over shortcuts
- realism over â€œtoyâ€ examples

Design decisions reflect real-world AWS tradeoffs rather than tutorial-style shortcuts.
